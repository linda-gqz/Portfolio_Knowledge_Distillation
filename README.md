# Knowledge Distillation for Efficiency


## Abstract
As deep learning has been widely used and become increasingly important to our daily life, the efficiency of deploying a complex neural network for classification or prediction on a small device stands out as one of the main concerns for developers and researchers. In reality, a problem arises when we train a network with high capacity source but fail to compress it to smaller devices, such as phones and tablets, due its cumbersome structure. In such situations, we want to distill knowledge to keep most of the networkâ€™s performance but training a much smaller and simpler network instead. One way to do this is to transfer knowledge learned by the cumbersome network, called a teacher network, to a simple network, called a student network. In order for the student network to carry the knowledge, mainly the uncertainty of the classification, instead of predicting the real classes, called hard labels, we train a student network to predict a weighted result of the hard labels and the softened probability of each class, called soft labels. The loss function is therefore a combination of the loss for the hard labels and the soft labels. A more practical situation occurs when the training data of the teacher network is not accessible due to privacy concern or company rivalry. In this case, we perform zero-shot knowledge distillation, in which we train the student network by data impressions, generated by the distribution of the softmax outputs of the teacher network and the class similarity matrix. In this project, I will explore knowledge distillation by expalining the algorithm of knowledge distillation with and without teacher network's training set and the impact of temperature and evaluating it by comparing the performance of the networks.

##### Keywords: teacher network, student network, hard labels, soft labels, temperature
